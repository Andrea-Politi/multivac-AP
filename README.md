# multivac-AP
simple python app

based on https://github.com/Unbabel/devops-coding-challenge

### Planning


Before starting building the stack, it is necessary to desing 1 or 2 possible scenarios in terms of infrastructure architecture for a live system:
one solution can be as follow
- An ELB load balancer rotating requests in a round robin fashion way
- 2 AWS EC2 instances on a Public Network with a webserver (i.e. NGINX) configured in two different availability zones within the same region
- 2 or more EC2 application instances in a private network configured within an AutoScaling group (i.e. between 2 and 3) with policies to start/stop instances based on CloundWatch metrics
- a DB tier including both MongoDB and Redis in a single instance (for session persistence), plus one or more read-replicas

* Security
there are several points to consider in order to make the whole stack reasonably secure:

** network security **
use security groups (and/or network ACL) to allow traffic only on listening ports; for example for the webservers in the DMZ zone it will be 80,443 from anywhere, and 22 for management purposes, maybe restricted to the public IP or network of the company.
On the other end, the application tier security group (or iptables chains) should be configured only to accept and forward requests from the private network interface of the webservers to the application port 5000 in a stateful way (default for security groups)

** infrastructure security **

Each single instance should be regularly patched to avoid common exploits and vulnerabilities. Patches and updates should be applied to the CI plugins as well.

** application security **

Some sort of authentication can be applied to the Flask's endpoints as well, at least to the POST and DELETE ones (i.e. http basic auth) even if the data is sent in clear text, the encryption can/should be managed elsewhere, like on the NGINX servers enabling SSL certs.
Plus the app servers should be tested against any possible common SQL injection and XSS techniques.

* Scalability

Given the architecture described above, the most obvious way to scale such architecture horizontally would be done via AutoScaling Groups and Launch Configuration.
Anyway, since the application is deployed inside a Docker container - which could be used even in a production environment - another way to scale can be managed using orchestration tools like Kubernetes, Docker Swarm, Apache Mesos (+Marathon) or furthermore AWS services such ECS. Describing in details which one to use is beyond the scope of this test. Another solution could be using a combination of the above; i.e. a Kubernetes cluster (which in my opinion is the best solution) with Docker images deployed under ECR (container registry service in AWS) to make the image available from anywhere using only their Amazion Resource Name.

* Logging

The application that run inside the container is configured to log on two different files on the host machine (under /var/log/app/ directory), one for the server, the other one for the worker. Further improvement can be configuring a logstash pipeline file to get these files as input, apply common filters/plugin to them and send the output to a kibana dashboard for graphs and statistics.

* Monitoring

The most common and easy way would be to install elsewhere within the same subnet a Nagios/Zabbix/Icinga2 or any other SNMP based monitoring server to launch usual checks on services and health on the instances. But since we are describing an application stack deployed under AWS, even better would be to use CloudWatch and ELB heartbeat on the load balancer.

* Automation

a pretty basic Ansible playbook is already implemented in the solution deployed.
Said that, it can be configured in many different ways to install just piece of software or any other common admin tasks in different machines and different tiers.
For the purpose of the test I just tried to keep it simple/

*** Installation ***

to make a fresh installation to any wanted host proceed as follow:

From a host with Ansible installed edit the host file provided in the release inside the target section (default is 'web' )
```
ansible-playbook basic.yml
```
the target machine requires python as prerequisite. This will install the basic packages, mongodb, redis-server, jenkins server, etc
```
ansible-playbook app.yml
```
this will build the docker image and start it. That's it!

*** Jenkins ***
The default Jenkins installation needs to be configured; once the first ansible-playbook has been completed without errors, connect to: http://server_ip:8080
it will first require the temporary admin password (autogenerated) which can be found in the log file /var/log/jenkins/jenkins.log
The plugin needs to be installed for the first run, at very least the Git plugin is required for polling.
There are two jobs currently configured on the target machine: repo_build and app_check.
The first is triggered after every commit is made to this repo. It will pull the changes, delete old containers and images, build a new image and start again the container.
The second is triggered after any successfull run of repo_build and it will test the /data and /zzz/ (delete) endpoints, not the GET!

*** Docker ***
The Dockerfile is built on the alpine OS image, which is the lightest possible. Everytime is built it will download all the python dependencies, run 'pip install' against requirements.txt, map the log files and run the Flask server and the worker. The container is bond to the host network, so that is more easy to manage the connections with the application (using just the host_ip on port 5000 and the DBs connections as well.
Basic commands:
check, stop, remove the container
```
sudo docker ps -a
sudo docker inspect app
sudo docker stop app
sudo docker rm app
```
start manually, execute a shell inside the container, image build, delete
```
sudo docker -d run --name app --net='host' -v /app/logs/:/var/log/app -it alpine_img
sudo docker exec -it app /bin/sh
sudo docker build -t alpine_img /path/to/Dockerfile
```

*** config.py ***
It's currently configured for a single machine installation, so both mongodb and redis-server connections are pointing to localhost; change them in case of multi-tier installation

*** views.py ***
I added the endpoint /zzz/ - which didn't exist before - It will delete one record from the db.collection

*** post-commit ***
This file may or may not be useful for notify Jenkins whenever a 'git commit' is launched from the command line. In case it needs to be filled with the target JENKINS_URL var and copied under $GIT_PROJECT/.git/hooks/

*** manually test the app ***

```
curl -X GET http://host_ip:5000/multivac

curl -X POST -d data="whatever I want to post.." http://host_ip:5000/multivac/data

curl -X DELETE http://host_ip:5000/multivac/zzz/

```

